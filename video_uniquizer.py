import cv2
import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
from moviepy.editor import VideoFileClip, CompositeVideoClip
import random
import os
from typing import Tuple, List, Optional
import json
from tqdm import tqdm
import logging

# VidGear fallback
try:
    from vidgear.gears import WriteGear
    VIDGEAR_AVAILABLE = True
except ImportError:
    VIDGEAR_AVAILABLE = False
    print("‚ö†Ô∏è VidGear not available, using MoviePy only")


class VideoUniquizer:
    """
    –ù–µ–π—Ä–æ–Ω–Ω–∞—è —Å–µ—Ç—å –¥–ª—è —É–Ω–∏–∫–∞–ª–∏–∑–∞—Ü–∏–∏ –≤–∏–¥–µ–æ —á–µ—Ä–µ–∑ –Ω–µ–∑–∞–º–µ—Ç–Ω—ã–µ –∏–∑–º–µ–Ω–µ–Ω–∏—è
    """
    
    def __init__(self, device: str = 'auto'):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —É–Ω–∏–∫–∞–ª–∏–∑–∞—Ç–æ—Ä–∞ –≤–∏–¥–µ–æ
        
        Args:
            device: –£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ ('cpu', 'cuda', 'auto')
        """
        if device == 'auto':
            self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        else:
            self.device = torch.device(device)
        
        print(f"–ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {self.device}")
        
        # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è –∑–∞–º–µ—Ç–Ω–æ–π —É–Ω–∏–∫–∞–ª–∏–∑–∞—Ü–∏–∏
        self.speed_range = (0.95, 1.05)  # –ó–∞–º–µ—Ç–Ω–æ–µ –∏–∑–º–µ–Ω–µ–Ω–∏–µ —Å–∫–æ—Ä–æ—Å—Ç–∏
        self.brightness_range = (-15, 15)  # –ó–∞–º–µ—Ç–Ω—ã–µ –∏–∑–º–µ–Ω–µ–Ω–∏—è —è—Ä–∫–æ—Å—Ç–∏
        self.contrast_range = (0.9, 1.1)  # –ó–∞–º–µ—Ç–Ω—ã–µ –∏–∑–º–µ–Ω–µ–Ω–∏—è –∫–æ–Ω—Ç—Ä–∞—Å—Ç–∞
        self.saturation_range = (0.9, 1.1)  # –ó–∞–º–µ—Ç–Ω—ã–µ –∏–∑–º–µ–Ω–µ–Ω–∏—è –Ω–∞—Å—ã—â–µ–Ω–Ω–æ—Å—Ç–∏
        
        # –≠—Ñ—Ñ–µ–∫—Ç—ã –≤ —Å—Ç–∏–ª–µ Instagram (–±–æ–ª–µ–µ –∑–∞–º–µ—Ç–Ω—ã–µ)
        self.social_effects = {
            'vintage': {'warmth': 0.9, 'vignette': 0.2, 'grain': 0.1},
            'dramatic': {'contrast': 1.15, 'shadows': 0.8, 'highlights': 1.2},
            'soft': {'blur': 0.5, 'brightness': 5, 'saturation': 0.9},
            'vibrant': {'saturation': 1.2, 'vibrance': 1.15, 'clarity': 1.1}
        }
        
        # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ Instagram —Ñ–∏–ª—å—Ç—Ä—ã
        self.instagram_filters = {
            'vintage': {'warmth': 0.9, 'vignette': 0.2, 'grain': 0.1},
            'dramatic': {'contrast': 1.15, 'shadows': 0.8, 'highlights': 1.2},
            'soft': {'blur': 0.5, 'brightness': 5, 'saturation': 0.9},
            'vibrant': {'saturation': 1.2, 'vibrance': 1.15, 'clarity': 1.1}
        }
        
    def apply_temporal_effects(self, video_path: str, output_path: str) -> str:
        """
        –ü—Ä–∏–º–µ–Ω—è–µ—Ç –≤—Ä–µ–º–µ–Ω–Ω—ã–µ —ç—Ñ—Ñ–µ–∫—Ç—ã (—Å–∫–æ—Ä–æ—Å—Ç—å, –æ–±—Ä–µ–∑–∫–∞)
        """
        clip = VideoFileClip(video_path)
        
        # –°–ª—É—á–∞–π–Ω–æ–µ –∏–∑–º–µ–Ω–µ–Ω–∏–µ —Å–∫–æ—Ä–æ—Å—Ç–∏
        speed_factor = random.uniform(*self.speed_range)
        new_duration = clip.duration / speed_factor
        
        # –°–ª—É—á–∞–π–Ω–∞—è –æ–±—Ä–µ–∑–∫–∞ (—É–±–∏—Ä–∞–µ–º 1-5% –æ—Ç –Ω–∞—á–∞–ª–∞ –∏ –∫–æ–Ω—Ü–∞)
        trim_start = random.uniform(0, clip.duration * 0.05)
        trim_end = random.uniform(0, clip.duration * 0.05)
        
        # –ü—Ä–∏–º–µ–Ω—è–µ–º –∏–∑–º–µ–Ω–µ–Ω–∏—è
        processed_clip = clip.subclip(trim_start, clip.duration - trim_end)
        processed_clip = processed_clip.fx(lambda clip: clip.speedx(speed_factor))
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º
        processed_clip.write_videofile(
            output_path, 
            codec='libx264', 
            audio_codec='aac',
            ffmpeg_params=['-preset', 'fast', '-crf', '23', '-threads', '2'],
            verbose=False,
            logger=None
        )
        processed_clip.close()
        clip.close()
        
        return output_path
    
    def apply_visual_effects(self, video_path: str, output_path: str) -> str:
        """
        –ü—Ä–∏–º–µ–Ω—è–µ—Ç –≤–∏–∑—É–∞–ª—å–Ω—ã–µ —ç—Ñ—Ñ–µ–∫—Ç—ã (—è—Ä–∫–æ—Å—Ç—å, –∫–æ–Ω—Ç—Ä–∞—Å—Ç, –Ω–∞—Å—ã—â–µ–Ω–Ω–æ—Å—Ç—å) —Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º –∞—É–¥–∏–æ
        """
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –≤–∏–¥–µ–æ —Å –∞—É–¥–∏–æ
        clip = VideoFileClip(video_path)
        
        # –°–ª—É—á–∞–π–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–æ–≤
        brightness_delta = random.randint(*self.brightness_range)
        contrast_alpha = random.uniform(*self.contrast_range)
        saturation_alpha = random.uniform(*self.saturation_range)
        
        print(f"–ü—Ä–∏–º–µ–Ω—è–µ–º —ç—Ñ—Ñ–µ–∫—Ç—ã: —è—Ä–∫–æ—Å—Ç—å={brightness_delta}, –∫–æ–Ω—Ç—Ä–∞—Å—Ç={contrast_alpha:.2f}, –Ω–∞—Å—ã—â–µ–Ω–Ω–æ—Å—Ç—å={saturation_alpha:.2f}")
        
        # –ü—Ä–∏–º–µ–Ω—è–µ–º —ç—Ñ—Ñ–µ–∫—Ç—ã –∫ –∫–∞–∂–¥–æ–º—É –∫–∞–¥—Ä—É
        def apply_effect(get_frame, t):
            frame = get_frame(t)
            return self._apply_frame_effects(frame, brightness_delta, contrast_alpha, saturation_alpha)
        
        # –°–æ–∑–¥–∞–µ–º –Ω–æ–≤—ã–π –∫–ª–∏–ø —Å —ç—Ñ—Ñ–µ–∫—Ç–∞–º–∏
        processed_clip = clip.fl(apply_effect)
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Å –∞—É–¥–∏–æ
        processed_clip.write_videofile(
            output_path, 
            codec='libx264', 
            audio_codec='aac',
            temp_audiofile='temp-audio.m4a',
            remove_temp=True,
            ffmpeg_params=[
                '-preset', 'fast',  # –ë—ã—Å—Ç—Ä–∞—è –∫–æ–¥–∏—Ä–æ–≤–∫–∞
                '-crf', '23',       # –ö–∞—á–µ—Å—Ç–≤–æ
                '-maxrate', '2M',   # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π –±–∏—Ç—Ä–µ–π—Ç
                '-bufsize', '4M',   # –†–∞–∑–º–µ—Ä –±—É—Ñ–µ—Ä–∞
                '-threads', '2',    # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ—Ç–æ–∫–æ–≤
                '-movflags', '+faststart'  # –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –¥–ª—è —Å—Ç—Ä–∏–º–∏–Ω–≥–∞
            ],
            verbose=False,
            logger=None
        )
        
        # –ó–∞–∫—Ä—ã–≤–∞–µ–º –∫–ª–∏–ø—ã
        processed_clip.close()
        clip.close()
        
        return output_path
    
    def _apply_frame_effects(self, frame: np.ndarray, brightness: int, 
                           contrast: float, saturation: float) -> np.ndarray:
        """
        –ü—Ä–∏–º–µ–Ω—è–µ—Ç —ç—Ñ—Ñ–µ–∫—Ç—ã –∫ –æ—Ç–¥–µ–ª—å–Ω–æ–º—É –∫–∞–¥—Ä—É
        """
        # –Ø—Ä–∫–æ—Å—Ç—å
        frame = cv2.convertScaleAbs(frame, alpha=1, beta=brightness)
        
        # –ö–æ–Ω—Ç—Ä–∞—Å—Ç
        frame = cv2.convertScaleAbs(frame, alpha=contrast, beta=0)
        
        # –ù–∞—Å—ã—â–µ–Ω–Ω–æ—Å—Ç—å
        hsv = cv2.cvtColor(frame, cv2.COLOR_BGR2HSV)
        hsv[:, :, 1] = cv2.convertScaleAbs(hsv[:, :, 1], alpha=saturation)
        frame = cv2.cvtColor(hsv, cv2.COLOR_HSV2BGR)
        
        # –°–ª—É—á–∞–π–Ω—ã–π —à—É–º (–æ—á–µ–Ω—å —Å–ª–∞–±—ã–π)
        noise = np.random.normal(0, 1, frame.shape).astype(np.uint8)
        frame = cv2.add(frame, noise)
        
        return frame
    
    def apply_neural_effects(self, video_path: str, output_path: str) -> str:
        """
        –ü—Ä–∏–º–µ–Ω—è–µ—Ç –Ω–µ–π—Ä–æ—Å–µ—Ç–µ–≤—ã–µ —ç—Ñ—Ñ–µ–∫—Ç—ã –¥–ª—è —É–Ω–∏–∫–∞–ª–∏–∑–∞—Ü–∏–∏
        """
        cap = cv2.VideoCapture(video_path)
        
        # –ü–æ–ª—É—á–∞–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –≤–∏–¥–µ–æ
        fps = int(cap.get(cv2.CAP_PROP_FPS))
        width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
        height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
        
        # –°–æ–∑–¥–∞–µ–º writer –¥–ª—è –≤—ã—Ö–æ–¥–Ω–æ–≥–æ –≤–∏–¥–µ–æ
        fourcc = cv2.VideoWriter_fourcc(*'H264')
        out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))
        
        print("–ü—Ä–∏–º–µ–Ω—è–µ–º –Ω–µ–π—Ä–æ—Å–µ—Ç–µ–≤—ã–µ —ç—Ñ—Ñ–µ–∫—Ç—ã...")
        
        frame_count = 0
        with tqdm(total=total_frames, desc="–ù–µ–π—Ä–æ—Å–µ—Ç–µ–≤–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞") as pbar:
            while True:
                ret, frame = cap.read()
                if not ret:
                    break
                
                # –ü—Ä–∏–º–µ–Ω—è–µ–º –Ω–µ–π—Ä–æ—Å–µ—Ç–µ–≤—ã–µ —ç—Ñ—Ñ–µ–∫—Ç—ã
                processed_frame = self._apply_neural_frame_effects(frame)
                
                out.write(processed_frame)
                frame_count += 1
                pbar.update(1)
        
        cap.release()
        out.release()
        
        return output_path
    
    def _apply_neural_frame_effects(self, frame: np.ndarray) -> np.ndarray:
        """
        –ü—Ä–∏–º–µ–Ω—è–µ—Ç –Ω–µ–π—Ä–æ—Å–µ—Ç–µ–≤—ã–µ —ç—Ñ—Ñ–µ–∫—Ç—ã –∫ –∫–∞–¥—Ä—É
        """
        # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ —Ç–µ–Ω–∑–æ—Ä
        frame_tensor = torch.from_numpy(frame).float().permute(2, 0, 1).unsqueeze(0) / 255.0
        frame_tensor = frame_tensor.to(self.device)
        
        # –°–ª—É—á–∞–π–Ω—ã–µ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏–∏
        with torch.no_grad():
            # –°–ª—É—á–∞–π–Ω–æ–µ –∏–∑–º–µ–Ω–µ–Ω–∏–µ –≥–∞–º–º—ã
            gamma = random.uniform(0.9, 1.1)
            frame_tensor = torch.pow(frame_tensor, gamma)
            
            # –°–ª—É—á–∞–π–Ω–æ–µ –∏–∑–º–µ–Ω–µ–Ω–∏–µ —Ü–≤–µ—Ç–æ–≤–æ–≥–æ –±–∞–ª–∞–Ω—Å–∞
            color_shift = torch.rand(3, 1, 1).to(self.device) * 0.1 - 0.05
            frame_tensor = frame_tensor + color_shift
            frame_tensor = torch.clamp(frame_tensor, 0, 1)
            
            # –°–ª—É—á–∞–π–Ω–æ–µ —Ä–∞–∑–º—ã—Ç–∏–µ (–æ—á–µ–Ω—å —Å–ª–∞–±–æ–µ)
            if random.random() < 0.3:
                kernel_size = random.choice([3, 5])
                # –°–æ–∑–¥–∞–µ–º —è–¥—Ä–æ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –∫–∞–Ω–∞–ª–∞ –æ—Ç–¥–µ–ª—å–Ω–æ
                blur_kernel = torch.ones(3, 1, kernel_size, kernel_size).to(self.device) / (kernel_size * kernel_size)
                frame_tensor = F.conv2d(frame_tensor, blur_kernel, padding=kernel_size//2, groups=3)
        
        # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –æ–±—Ä–∞—Ç–Ω–æ –≤ numpy
        frame_tensor = frame_tensor.squeeze(0).permute(1, 2, 0).cpu().numpy()
        frame_tensor = (frame_tensor * 255).astype(np.uint8)
        
        return frame_tensor
    
    def apply_social_effects(self, video_path: str, output_path: str) -> str:
        """
        –ü—Ä–∏–º–µ–Ω—è–µ—Ç –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ —ç—Ñ—Ñ–µ–∫—Ç—ã –≤ —Å—Ç–∏–ª–µ —Å–æ—Ü–∏–∞–ª—å–Ω—ã—Ö —Å–µ—Ç–µ–π (—Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º –∞—É–¥–∏–æ)
        """
        try:
            return self._apply_social_effects_moviepy(video_path, output_path)
        except Exception as e:
            print(f"‚ö†Ô∏è MoviePy failed: {e}")
            if VIDGEAR_AVAILABLE:
                print("üîÑ Trying VidGear fallback...")
                return self._apply_social_effects_vidgear(video_path, output_path)
            else:
                print("‚ùå No fallback available, re-raising error")
                raise e
    
    def _apply_social_effects_moviepy(self, video_path: str, output_path: str) -> str:
        """
        MoviePy implementation of social effects
        """
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –≤–∏–¥–µ–æ —Å –∞—É–¥–∏–æ
        clip = VideoFileClip(video_path)
        
        # –°–ª—É—á–∞–π–Ω–æ –≤—ã–±–∏—Ä–∞–µ–º —Å—Ç–∏–ª—å —ç—Ñ—Ñ–µ–∫—Ç–∞
        effect_style = random.choice(list(self.social_effects.keys()))
        effect_params = self.social_effects[effect_style]
        
        print(f"–ü—Ä–∏–º–µ–Ω—è–µ–º —ç—Ñ—Ñ–µ–∫—Ç '{effect_style}': {effect_params}")
        
        # –ü—Ä–∏–º–µ–Ω—è–µ–º —ç—Ñ—Ñ–µ–∫—Ç—ã –∫ –∫–∞–∂–¥–æ–º—É –∫–∞–¥—Ä—É
        def apply_effect(get_frame, t):
            frame = get_frame(t)
            return self._apply_social_frame_effects(frame, effect_style, effect_params)
        
        # –°–æ–∑–¥–∞–µ–º –Ω–æ–≤—ã–π –∫–ª–∏–ø —Å —ç—Ñ—Ñ–µ–∫—Ç–∞–º–∏
        processed_clip = clip.fl(apply_effect)
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Å –∞—É–¥–∏–æ
        processed_clip.write_videofile(
            output_path, 
            codec='libx264', 
            audio_codec='aac',
            temp_audiofile='temp-audio.m4a',
            remove_temp=True,
            ffmpeg_params=[
                '-preset', 'fast',  # –ë—ã—Å—Ç—Ä–∞—è –∫–æ–¥–∏—Ä–æ–≤–∫–∞
                '-crf', '23',       # –ö–∞—á–µ—Å—Ç–≤–æ
                '-maxrate', '2M',   # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π –±–∏—Ç—Ä–µ–π—Ç
                '-bufsize', '4M',   # –†–∞–∑–º–µ—Ä –±—É—Ñ–µ—Ä–∞
                '-threads', '2',    # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ—Ç–æ–∫–æ–≤
                '-movflags', '+faststart'  # –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –¥–ª—è —Å—Ç—Ä–∏–º–∏–Ω–≥–∞
            ],
            verbose=False,
            logger=None
        )
        
        # –ó–∞–∫—Ä—ã–≤–∞–µ–º –∫–ª–∏–ø—ã
        processed_clip.close()
        clip.close()
        
        return output_path
    
    def _apply_social_effects_vidgear(self, video_path: str, output_path: str) -> str:
        """
        VidGear fallback implementation for social effects
        """
        print("üé¨ Using VidGear for video processing...")
        
        # –û—Ç–∫—Ä—ã–≤–∞–µ–º –≤–∏–¥–µ–æ —Å –ø–æ–º–æ—â—å—é OpenCV
        cap = cv2.VideoCapture(video_path)
        if not cap.isOpened():
            raise ValueError(f"Cannot open video: {video_path}")
        
        # –ü–æ–ª—É—á–∞–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –≤–∏–¥–µ–æ
        fps = int(cap.get(cv2.CAP_PROP_FPS))
        width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
        height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
        
        print(f"üìπ Video info: {width}x{height} @ {fps}fps, {total_frames} frames")
        
        # –ù–∞—Å—Ç—Ä–æ–π–∫–∏ VidGear
        output_params = {
            "-vcodec": "libx264",
            "-preset": "fast",
            "-crf": "23",
            "-maxrate": "2M",
            "-bufsize": "4M",
            "-threads": "2",
            "-movflags": "+faststart"
        }
        
        # –°–ª—É—á–∞–π–Ω–æ –≤—ã–±–∏—Ä–∞–µ–º —Å—Ç–∏–ª—å —ç—Ñ—Ñ–µ–∫—Ç–∞
        effect_style = random.choice(list(self.social_effects.keys()))
        effect_params = self.social_effects[effect_style]
        
        print(f"–ü—Ä–∏–º–µ–Ω—è–µ–º —ç—Ñ—Ñ–µ–∫—Ç '{effect_style}': {effect_params}")
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º VidGear writer
        writer = WriteGear(output_filename=output_path, logging=False, **output_params)
        
        frame_count = 0
        try:
            while True:
                ret, frame = cap.read()
                if not ret:
                    break
                
                # –ü—Ä–∏–º–µ–Ω—è–µ–º —ç—Ñ—Ñ–µ–∫—Ç—ã –∫ –∫–∞–¥—Ä—É
                processed_frame = self._apply_social_frame_effects(frame, effect_style, effect_params)
                
                # –ó–∞–ø–∏—Å—ã–≤–∞–µ–º –∫–∞–¥—Ä
                writer.write(processed_frame)
                
                frame_count += 1
                if frame_count % 30 == 0:  # Progress every 30 frames
                    print(f"üìä Processed {frame_count}/{total_frames} frames ({frame_count/total_frames*100:.1f}%)")
        
        finally:
            # –ó–∞–∫—Ä—ã–≤–∞–µ–º –≤—Å–µ
            cap.release()
            writer.close()
        
        print(f"‚úÖ VidGear processing completed: {frame_count} frames")
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ —Ñ–∞–π–ª —Å–æ–∑–¥–∞–Ω –∏ –Ω–µ –ø—É—Å—Ç–æ–π
        if not os.path.exists(output_path) or os.path.getsize(output_path) == 0:
            raise ValueError("VidGear output file is empty or doesn't exist")
        
        return output_path
    
    def _apply_social_frame_effects(self, frame: np.ndarray, style: str, params: dict) -> np.ndarray:
        """
        –ü—Ä–∏–º–µ–Ω—è–µ—Ç —ç—Ñ—Ñ–µ–∫—Ç—ã —Å–æ—Ü–∏–∞–ª—å–Ω—ã—Ö —Å–µ—Ç–µ–π –∫ –∫–∞–¥—Ä—É
        """
        if style == 'vintage':
            # –í–∏–Ω—Ç–∞–∂–Ω—ã–π —ç—Ñ—Ñ–µ–∫—Ç: —Ç–µ–ø–ª–æ—Ç–∞, –≤–∏–Ω—å–µ—Ç–∫–∞, –∑–µ—Ä–Ω–æ
            frame = self._apply_vintage_effect(frame, params)
        elif style == 'dramatic':
            # –î—Ä–∞–º–∞—Ç–∏—á–µ—Å–∫–∏–π —ç—Ñ—Ñ–µ–∫—Ç: –∫–æ–Ω—Ç—Ä–∞—Å—Ç, —Ç–µ–Ω–∏, –±–ª–∏–∫–∏
            frame = self._apply_dramatic_effect(frame, params)
        elif style == 'soft':
            # –ú—è–≥–∫–∏–π —ç—Ñ—Ñ–µ–∫—Ç: —Ä–∞–∑–º—ã—Ç–∏–µ, —è—Ä–∫–æ—Å—Ç—å
            frame = self._apply_soft_effect(frame, params)
        elif style == 'vibrant':
            # –Ø—Ä–∫–∏–π —ç—Ñ—Ñ–µ–∫—Ç: –Ω–∞—Å—ã—â–µ–Ω–Ω–æ—Å—Ç—å, –≤–∏–±—Ä–∞—Ü–∏—è
            frame = self._apply_vibrant_effect(frame, params)
        
        return frame
    
    def _apply_vintage_effect(self, frame: np.ndarray, params: dict) -> np.ndarray:
        """–í–∏–Ω—Ç–∞–∂–Ω—ã–π —ç—Ñ—Ñ–µ–∫—Ç –∫–∞–∫ –≤ Instagram"""
        # –°–æ–∑–¥–∞–µ–º –∫–æ–ø–∏—é –¥–ª—è –∏–∑–º–µ–Ω–µ–Ω–∏—è
        frame = frame.copy()
        
        # –¢–µ–ø–ª–æ—Ç–∞ (—Å–¥–≤–∏–≥ –≤ —Å—Ç–æ—Ä–æ–Ω—É –æ—Ä–∞–Ω–∂–µ–≤–æ–≥–æ)
        warmth = params['warmth']
        frame[:, :, 0] = np.clip(frame[:, :, 0] * warmth, 0, 255)  # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º –∫—Ä–∞—Å–Ω—ã–π
        frame[:, :, 2] = np.clip(frame[:, :, 2] * (2 - warmth), 0, 255)  # –£–º–µ–Ω—å—à–∞–µ–º —Å–∏–Ω–∏–π
        
        # –í–∏–Ω—å–µ—Ç–∫–∞ (–∑–∞—Ç–µ–º–Ω–µ–Ω–∏–µ –∫—Ä–∞–µ–≤)
        vignette = params['vignette']
        h, w = frame.shape[:2]
        y, x = np.ogrid[:h, :w]
        center_x, center_y = w // 2, h // 2
        mask = np.sqrt((x - center_x)**2 + (y - center_y)**2)
        mask = mask / mask.max()
        vignette_mask = 1 - (mask * vignette)
        frame = frame * vignette_mask[:, :, np.newaxis]
        
        # –ó–µ—Ä–Ω–æ (—à—É–º)
        grain = params['grain']
        noise = np.random.normal(0, grain * 25, frame.shape)
        frame = np.clip(frame + noise, 0, 255)
        
        return frame.astype(np.uint8)
    
    def _apply_dramatic_effect(self, frame: np.ndarray, params: dict) -> np.ndarray:
        """–î—Ä–∞–º–∞—Ç–∏—á–µ—Å–∫–∏–π —ç—Ñ—Ñ–µ–∫—Ç –∫–∞–∫ –≤ TikTok"""
        # –°–æ–∑–¥–∞–µ–º –∫–æ–ø–∏—é –¥–ª—è –∏–∑–º–µ–Ω–µ–Ω–∏—è
        frame = frame.copy()
        
        # –ö–æ–Ω—Ç—Ä–∞—Å—Ç
        contrast = params['contrast']
        frame = cv2.convertScaleAbs(frame, alpha=contrast, beta=0)
        
        # –¢–µ–Ω–∏ –∏ –±–ª–∏–∫–∏
        shadows = params['shadows']
        highlights = params['highlights']
        
        # –ü—Ä–∏–º–µ–Ω—è–µ–º –∫—Ä–∏–≤—ã–µ (—É–ø—Ä–æ—â–µ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è)
        frame = frame.astype(np.float32)
        frame = np.clip(frame * highlights, 0, 255)
        frame = np.clip(frame * shadows, 0, 255)
        
        return frame.astype(np.uint8)
    
    def _apply_soft_effect(self, frame: np.ndarray, params: dict) -> np.ndarray:
        """–ú—è–≥–∫–∏–π —ç—Ñ—Ñ–µ–∫—Ç –∫–∞–∫ –≤ YouTube Shorts"""
        # –°–æ–∑–¥–∞–µ–º –∫–æ–ø–∏—é –¥–ª—è –∏–∑–º–µ–Ω–µ–Ω–∏—è
        frame = frame.copy()
        
        # –°–ª–∞–±–æ–µ —Ä–∞–∑–º—ã—Ç–∏–µ
        blur = params['blur']
        if blur > 0:
            kernel_size = int(blur * 3) * 2 + 1  # –ù–µ—á–µ—Ç–Ω–æ–µ —á–∏—Å–ª–æ
            frame = cv2.GaussianBlur(frame, (kernel_size, kernel_size), blur)
        
        # –Ø—Ä–∫–æ—Å—Ç—å
        brightness = params['brightness']
        frame = cv2.convertScaleAbs(frame, alpha=1, beta=brightness)
        
        # –ù–∞—Å—ã—â–µ–Ω–Ω–æ—Å—Ç—å
        saturation = params['saturation']
        hsv = cv2.cvtColor(frame, cv2.COLOR_BGR2HSV)
        hsv[:, :, 1] = cv2.convertScaleAbs(hsv[:, :, 1], alpha=saturation)
        frame = cv2.cvtColor(hsv, cv2.COLOR_HSV2BGR)
        
        return frame
    
    def _apply_vibrant_effect(self, frame: np.ndarray, params: dict) -> np.ndarray:
        """–Ø—Ä–∫–∏–π —ç—Ñ—Ñ–µ–∫—Ç –∫–∞–∫ –≤ Instagram Stories"""
        # –°–æ–∑–¥–∞–µ–º –∫–æ–ø–∏—é –¥–ª—è –∏–∑–º–µ–Ω–µ–Ω–∏—è
        frame = frame.copy()
        
        # –ù–∞—Å—ã—â–µ–Ω–Ω–æ—Å—Ç—å
        saturation = params['saturation']
        hsv = cv2.cvtColor(frame, cv2.COLOR_BGR2HSV)
        hsv[:, :, 1] = cv2.convertScaleAbs(hsv[:, :, 1], alpha=saturation)
        frame = cv2.cvtColor(hsv, cv2.COLOR_HSV2BGR)
        
        # –í–∏–±—Ä–∞—Ü–∏—è (—É—Å–∏–ª–µ–Ω–∏–µ —Ü–≤–µ—Ç–æ–≤)
        vibrance = params['vibrance']
        frame = cv2.convertScaleAbs(frame, alpha=vibrance, beta=0)
        
        # –ß–µ—Ç–∫–æ—Å—Ç—å (—É—Å–∏–ª–µ–Ω–∏–µ –∫—Ä–∞–µ–≤)
        clarity = params['clarity']
        if clarity > 1.0:
            # –ü—Ä–æ—Å—Ç–æ–µ —É—Å–∏–ª–µ–Ω–∏–µ –∫—Ä–∞–µ–≤
            gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
            edges = cv2.Laplacian(gray, cv2.CV_64F)
            edges = np.clip(edges, 0, 255).astype(np.uint8)
            edges = cv2.cvtColor(edges, cv2.COLOR_GRAY2BGR)
            frame = cv2.addWeighted(frame, 1.0, edges, (clarity - 1.0) * 0.3, 0)
        
        return frame
    
    def uniquize_video(self, input_path: str, output_path: str, 
                      effects: List[str] = None) -> str:
        """
        –û—Å–Ω–æ–≤–Ω–æ–π –º–µ—Ç–æ–¥ –¥–ª—è —É–Ω–∏–∫–∞–ª–∏–∑–∞—Ü–∏–∏ –≤–∏–¥–µ–æ —Å VidGear fallback
        
        Args:
            input_path: –ü—É—Ç—å –∫ –≤—Ö–æ–¥–Ω–æ–º—É –≤–∏–¥–µ–æ
            output_path: –ü—É—Ç—å –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞
            effects: –°–ø–∏—Å–æ–∫ —ç—Ñ—Ñ–µ–∫—Ç–æ–≤ –¥–ª—è –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è
            
        Returns:
            –ü—É—Ç—å –∫ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω–æ–º—É –≤–∏–¥–µ–æ
        """
        if effects is None:
            effects = ['temporal', 'social']  # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Å–æ—Ü–∏–∞–ª—å–Ω—ã–µ —ç—Ñ—Ñ–µ–∫—Ç—ã –≤–º–µ—Å—Ç–æ –Ω–µ–π—Ä–æ—Å–µ—Ç–µ–≤—ã—Ö
        
        print(f"–£–Ω–∏–∫–∞–ª–∏–∑–∞—Ü–∏—è –≤–∏–¥–µ–æ: {input_path}")
        print(f"–ü—Ä–∏–º–µ–Ω—è–µ–º—ã–µ —ç—Ñ—Ñ–µ–∫—Ç—ã: {effects}")
        
        temp_path = f"temp_{random.randint(1000, 9999)}.mp4"
        current_path = input_path
        
        try:
            # –ü—Ä–∏–º–µ–Ω—è–µ–º —ç—Ñ—Ñ–µ–∫—Ç—ã –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ
            for i, effect in enumerate(effects):
                if effect == 'temporal':
                    print("–ü—Ä–∏–º–µ–Ω—è–µ–º –≤—Ä–µ–º–µ–Ω–Ω—ã–µ —ç—Ñ—Ñ–µ–∫—Ç—ã...")
                    self.apply_temporal_effects(current_path, temp_path)
                elif effect == 'visual':
                    print("–ü—Ä–∏–º–µ–Ω—è–µ–º –≤–∏–∑—É–∞–ª—å–Ω—ã–µ —ç—Ñ—Ñ–µ–∫—Ç—ã...")
                    self.apply_visual_effects(current_path, temp_path)
                elif effect == 'neural':
                    print("–ü—Ä–∏–º–µ–Ω—è–µ–º –Ω–µ–π—Ä–æ—Å–µ—Ç–µ–≤—ã–µ —ç—Ñ—Ñ–µ–∫—Ç—ã...")
                    self.apply_neural_effects(current_path, temp_path)
                elif effect == 'social':
                    print("–ü—Ä–∏–º–µ–Ω—è–µ–º —ç—Ñ—Ñ–µ–∫—Ç—ã —Å–æ—Ü–∏–∞–ª—å–Ω—ã—Ö —Å–µ—Ç–µ–π...")
                    self.apply_social_effects(current_path, temp_path)
                
                # –û–±–Ω–æ–≤–ª—è–µ–º –ø—É—Ç—å –¥–ª—è —Å–ª–µ–¥—É—é—â–µ–≥–æ —ç—Ñ—Ñ–µ–∫—Ç–∞
                if i > 0:  # –£–¥–∞–ª—è–µ–º –ø—Ä–µ–¥—ã–¥—É—â–∏–π –≤—Ä–µ–º–µ–Ω–Ω—ã–π —Ñ–∞–π–ª
                    os.remove(current_path)
                current_path = temp_path
                
                if i < len(effects) - 1:  # –°–æ–∑–¥–∞–µ–º –Ω–æ–≤—ã–π –≤—Ä–µ–º–µ–Ω–Ω—ã–π —Ñ–∞–π–ª
                    temp_path = f"temp_{random.randint(1000, 9999)}.mp4"
            
            # –ü–µ—Ä–µ–∏–º–µ–Ω–æ–≤—ã–≤–∞–µ–º —Ñ–∏–Ω–∞–ª—å–Ω—ã–π —Ñ–∞–π–ª
            os.rename(current_path, output_path)
            print(f"–í–∏–¥–µ–æ —É—Å–ø–µ—à–Ω–æ —É–Ω–∏–∫–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–æ: {output_path}")
            
        except Exception as e:
            print(f"‚ö†Ô∏è MoviePy processing failed: {e}")
            if VIDGEAR_AVAILABLE:
                print("üîÑ Trying VidGear fallback for full video processing...")
                return self._uniquize_video_vidgear(input_path, output_path, effects)
            else:
                print("‚ùå No fallback available, re-raising error")
                # –û—á–∏—â–∞–µ–º –≤—Ä–µ–º–µ–Ω–Ω—ã–µ —Ñ–∞–π–ª—ã
                for temp_file in [temp_path, current_path]:
                    if os.path.exists(temp_file):
                        os.remove(temp_file)
                raise
        
        return output_path
    
    def _uniquize_video_vidgear(self, input_path: str, output_path: str, effects: List[str]) -> str:
        """
        VidGear fallback implementation for full video uniquization
        """
        print("üé¨ Using VidGear for full video uniquization...")
        
        # –û—Ç–∫—Ä—ã–≤–∞–µ–º –≤–∏–¥–µ–æ —Å –ø–æ–º–æ—â—å—é OpenCV
        cap = cv2.VideoCapture(input_path)
        if not cap.isOpened():
            raise ValueError(f"Cannot open video: {input_path}")
        
        # –ü–æ–ª—É—á–∞–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –≤–∏–¥–µ–æ
        fps = int(cap.get(cv2.CAP_PROP_FPS))
        width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
        height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
        
        print(f"üìπ Video info: {width}x{height} @ {fps}fps, {total_frames} frames")
        
        # –ù–∞—Å—Ç—Ä–æ–π–∫–∏ VidGear
        output_params = {
            "-vcodec": "libx264",
            "-preset": "fast",
            "-crf": "23",
            "-maxrate": "2M",
            "-bufsize": "4M",
            "-threads": "2",
            "-movflags": "+faststart"
        }
        
        # –°–ª—É—á–∞–π–Ω–æ –≤—ã–±–∏—Ä–∞–µ–º —Å—Ç–∏–ª—å —ç—Ñ—Ñ–µ–∫—Ç–∞
        effect_style = random.choice(list(self.social_effects.keys()))
        effect_params = self.social_effects[effect_style]
        
        print(f"–ü—Ä–∏–º–µ–Ω—è–µ–º —ç—Ñ—Ñ–µ–∫—Ç '{effect_style}': {effect_params}")
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º VidGear writer
        writer = WriteGear(output_filename=output_path, logging=False, **output_params)
        
        frame_count = 0
        try:
            while True:
                ret, frame = cap.read()
                if not ret:
                    break
                
                # –ü—Ä–∏–º–µ–Ω—è–µ–º —ç—Ñ—Ñ–µ–∫—Ç—ã –∫ –∫–∞–¥—Ä—É
                processed_frame = self._apply_social_frame_effects(frame, effect_style, effect_params)
                
                # –ó–∞–ø–∏—Å—ã–≤–∞–µ–º –∫–∞–¥—Ä
                writer.write(processed_frame)
                
                frame_count += 1
                if frame_count % 30 == 0:  # Progress every 30 frames
                    print(f"üìä Processed {frame_count}/{total_frames} frames ({frame_count/total_frames*100:.1f}%)")
        
        finally:
            # –ó–∞–∫—Ä—ã–≤–∞–µ–º –≤—Å–µ
            cap.release()
            writer.close()
        
        print(f"‚úÖ VidGear uniquization completed: {frame_count} frames")
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ —Ñ–∞–π–ª —Å–æ–∑–¥–∞–Ω –∏ –Ω–µ –ø—É—Å—Ç–æ–π
        if not os.path.exists(output_path) or os.path.getsize(output_path) == 0:
            raise ValueError("VidGear output file is empty or doesn't exist")
        
        return output_path


def main():
    """
    –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è VideoUniquizer
    """
    # –°–æ–∑–¥–∞–µ–º —ç–∫–∑–µ–º–ø–ª—è—Ä —É–Ω–∏–∫–∞–ª–∏–∑–∞—Ç–æ—Ä–∞
    uniquizer = VideoUniquizer()
    
    # –ü—É—Ç–∏ –∫ —Ñ–∞–π–ª–∞–º
    input_video = "vtec_idw_light.mp4"  # –ó–∞–º–µ–Ω–∏—Ç–µ –Ω–∞ –≤–∞—à —Ñ–∞–π–ª
    output_video = "uniquized_video.mp4"
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—É—â–µ—Å—Ç–≤–æ–≤–∞–Ω–∏–µ –≤—Ö–æ–¥–Ω–æ–≥–æ —Ñ–∞–π–ª–∞
    if not os.path.exists(input_video):
        print(f"–§–∞–π–ª {input_video} –Ω–µ –Ω–∞–π–¥–µ–Ω!")
        print("–ü–æ–º–µ—Å—Ç–∏—Ç–µ –≤–∞—à–µ –≤–∏–¥–µ–æ –≤ –ø–∞–ø–∫—É –ø—Ä–æ–µ–∫—Ç–∞ –∏ –ø–µ—Ä–µ–∏–º–µ–Ω—É–π—Ç–µ –µ–≥–æ –≤ 'input_video.mp4'")
        return
    
    try:
        # –£–Ω–∏–∫–∞–ª–∏–∑–∏—Ä—É–µ–º –≤–∏–¥–µ–æ
        result_path = uniquizer.uniquize_video(
            input_path=input_video,
            output_path=output_video,
            effects=['temporal', 'visual', 'neural']  # –í—Å–µ —ç—Ñ—Ñ–µ–∫—Ç—ã
        )
        
        print(f"‚úÖ –í–∏–¥–µ–æ —É—Å–ø–µ—à–Ω–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {result_path}")
        
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞: {e}")


if __name__ == "__main__":
    main()
